# LLM_LoRA-IA3
## Introduction
PEFT 提供了一種用於微調預訓練語言模型的有效方法，目的在於提高微調效率並減少所需的可訓練參數數量，其中 LoRA 和 IA3 是兩種與PEFT相關的方法，它們皆是通過不同的技術來達成一樣的目標。

- PEFT（Parameter-Efficient Fine-Tuning）： 
PEFT它是一個函式庫，可以有效地將大型預訓練模型去適應各種下游的應用程序，而無需微調模型的所有參數，透過僅微調少量或額外模型參數進而顯著降低計算和儲存成本，且它可以產生與完全微調模型一樣的性能!這樣能夠使得我們在硬體上訓練和儲存大型語言模型變得更加容易，且PEFT 與 Transformers、Diffusers 和 Accelerate 庫集成，更能提供更快更簡單的方法來載入、訓練和使用模型並進行推理，相較傳統的方法是針對每個底下的任務微調模型的所有參數，若使用此技術，訓練較少數量的提示參數或使用低秩自適應（LoRA）等重新參數化方法來減少可訓練參數的數量會更有效率、耗費更少時間。

  - LoRA (Layerwise Relevance Adaptive Scaling)：
LoRA它是一個有效訓練大型模型的方法，其是插入較小的可訓練矩陣，這些矩陣是微調期間要學習的增量權重矩陣的低秩分解能夠使預訓練模型的原始權重矩陣被凍結，並且在訓練期間僅更新較小的矩陣 這大大減少了可訓練參數的數量並進而減少了記憶體使用量和訓練時間 使模型能夠自適應地調整不同層的學習速率，從而提高微調效率和性能，此外 PEFT 函式庫支援其他幾種LoRA 變體，例如：Hadamard Product (LoHa)、Kronecker Product (LoKr) 和自適應低秩適應，這些方法都可以應用到其他模型中並完成像是語義分割、標記分類等任務 。

  - IA3(Importance Aware Activation Augmentation)：
IA3是將對模型的激活進行加權，即自注意力和編碼器與解碼器注意力塊中的鍵和值以及位置前饋網路的中間激活等乘以三個學習向量，使用這種方法引入的可訓練參數數量會比 LoRA 方法還要少，因為 LoRA 此法是引入了權重矩陣而不是向量，其 IA3 原始模型的參數保持凍結，並僅更新這些向量通過僅更新模型中的部分參數，因此對模型進行微調會更快速、更少花費、更有效率。

## What did I do
我將利用 LoRA 和 IA3 以上兩種技術於大型語言模型之中，進一步觀察其效能表現等等，接著更使用龐大的數據集輸入進模型，比較不同面向中語言模型的表現效果，其結論為：

LoRA輸出的結果： 輸入圖片的資料集，並運用 LoRA 的方法，發現整體訓練大約跑了快2小時，我觀察到利用 LoRA 方法，在 Loss 方面下降都有一定的幅度，在epoch 5的時候其訓練和驗證集的Loss已各別從 0.830000 和0.474696 降至 0.536700 與 0.339185，下降的速度很快 !

IA3輸出的結果： 輸入情緒分析資料集並運用 IA3 的方法，預設定為訓練 3 個 epoch，而每個 epoch 大致跑了快3分鐘，訓練整體跑完大致花了10分鐘左右，我觀察到 IA3 方法能夠使訓練的速度變得更快、花更少的時間，整個變的很有效率，而且在 Loss 方面 epoch 0 的時候 Loss 為 0.3926 epoch 1 的時候降至 0.0657 ，更在 epoch 2 的時候整體又降至 0.0414 Loss 下降的速度和幅度非常之大，比 LoRA 表現的更加出色 !

額外自己嘗試使用新的資料訓練並微調 IA3 方法的輸出結果：輸入Yelp Polarity情緒分析的餐廳評論資料集，並運用IA3的方法以及微調模型的參數，將 epochs 參數調整至 5 ，給更多的 epoch 以利訓練，而因為資料量較龐大所以每個 epoch 大約跑了快 1 2 分鐘，所有 epoch 跑完大致花了 1 個多小時左右，觀察到使用 IA3 方法一樣 能夠使訓練的速度變得更快更有效率，其在 Loss 方面 epoch 0 的時候已降到 Loss 為 0.1462 大發現這比上面原本使用較少的資料量和較少 epoch 的 I A3 實驗其輸出的 Loss 更低上面實驗於 epoch 0 的時候 Loss 為 0.3926 epoch 1 的時候降至 0.0841，更在epoch 2 的時候整體又降至 0.0648 ，最後 epoch 3 的時候為 0.0532 ，最終 epoch 4 的時候則為 0.0418，觀察出在大數據集以及更多 epoch 的情況之下， IA3 依然維持著很棒的表現。

**以上為 LoRA 和 IA3 以及使用額外數據集實驗的結果，並針對其各自呈現的效能和輸出與 Loss 等方面進行多方面的分析與整理，整體收穫到 IA3 表現的比 LoRA 好，且在額外自己使用新的資料訓練並微調 IA3 方法時更發現它表現效果也毫不遜色、更加良好，此項技術適合應用於大型語言模型之中，其可以大大降低分析與訓練大量資料集時所消耗的時間，更大幅提升了模型運行的效能。**
